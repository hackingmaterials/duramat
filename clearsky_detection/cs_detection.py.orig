
import warnings
import os

import numpy as np
import pandas as pd
from scipy import optimize
import pvlib

class ClearskyDetection(object):
    """Class for detecting clearsky based on NSRDB data."""

    def __init__(self, df, copy=True):
        """Initialize members.

        Arguments
        ---------
        nsrdb_df: pd.DataFrame
            NSRDB data.
        ground_df: pd.DataFrame
            Ground based data.
        """
        if copy:
            self.df = df.copy()
        else:
            self.df = df
        # if str(self.df.index.tz) != 'UTC':
        #     self.df.index = self.df.index.tz_convert('UTC')

    @classmethod
    def read_nsrdb_dir(cls, dir_path, timezone, keepers=['GHI', 'Clearsky GHI', 'Cloud Type'], file_ext='csv'):
        """Read directory of NSRDB files.

        *** NOTE ***
        This is hardcoded for the files I have.  It is not guaranteed to be general at all for
        SRRL/MDIC/etc data sources and files.

        Arguments
        ---------
        dir_path: str
            Path to directory of files.
        timezone: pytz.timezone
            Timezone for the dataframe indicies.
        file_ext, optional: str
            Filetype to specify for reading.

        Returns
        -------
        df: pd.DataFrame
            Contains all fields from files.
        """
        if file_ext.lower() not in ('csv'):
            raise NotImplementedError('Only accept CSV files at this time.')
        files = [os.path.join(dir_path, i) for i in os.listdir(dir_path) if i.endswith(file_ext)]
        df = pd.concat([pd.read_csv(f, header=2) for f in files])
        df.index = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute']])
        tmp_list = []
        for index in df.index:
            try:
                index.tz_localize(timezone)
                tmp_list.append(index)
            except:
                pass
        df = df[df.index.isin(tmp_list)]
        df.index = df.index.tz_localize(timezone)
        # df.index = df.index.tz_localize('UTC')
        # df.index = df.index.tz_convert(timezone)
        df = df.sort_index()
        df = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1], freq='30min')).fillna(0)
        df = df[~df.index.duplicated(keep='first')]
        df.index = df.index.tz_convert('UTC')
        return cls(df[keepers])

    @classmethod
    def read_snl_rtc(cls, file_w_path, timezone, keepers=['GHI']):
    # def read_snl_rtc(cls, file_w_path, timezone1, timezone2, keepers=['GHI']):
        """Read SNL RTC data into file.

        *** NOTE ***
        This is hardcoded for the files I have.  It is not guaranteed to be general at all for
        SRRL/MDIC/etc data sources and files.

        Arguments
        ---------
        file_w_path: str
            Path to file (absolute).
        timezone1: pytz.timezone or str
            Timezone for localization.
        timezone2: pytz.timezone or str
            Timezone to which indices will be converted.

        Returns
        -------
        df: pd.DataFrame
            Contains all fields from files.
        """
        df = pd.read_csv(file_w_path, parse_dates=['Date-Time'], index_col=['Date-Time'])
        tmp_list = []
        for index in df.index:
            try:
                index.tz_localize(timezone)
                tmp_list.append(index)
            except:
                pass
        df = df[df.index.isin(tmp_list)]
        df.index = df.index.tz_localize(timezone)
        df = df.sort_index()
        df = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1], freq='1min')).fillna(0)
        df = df[~df.index.duplicated(keep='first')]
        df['GHI'] = df['Global_Wm2']
        df.index = df.index.tz_convert('UTC')
        return cls(df[keepers])

    @classmethod
    def read_srrl_dir(cls, dir_path, timezone, file_ext='txt', keepers=['GHI']):
        """Read directory of SRRL files into a dataframe.

        *** NOTE ***
        This is hardcoded for the files I have.  It is not guaranteed to be general at all for
        SRRL/MDIC/etc data sources and files.

        Arguments
        ---------
        dir_path: str
            Path to directory of files.
        timezone: pytz.timezone
            Timezone for the dataframe indicies.
        file_ext, optional: str
            Filetype to specify for reading.

        Returns
        -------
        df: pd.DataFrame
            Contains all fields from files.
        """
        files = [os.path.join(dir_path, i) for i in os.listdir(dir_path) if i.endswith('txt')]
        df = pd.concat([pd.read_csv(f) for f in files])
        df.index = pd.to_datetime(df['DATE (MM/DD/YYYY)'] + ' ' + df['MST'], format='%m/%d/%Y %H:%M')
        tmp_list = []
        for index in df.index:
            try:
                index.tz_localize(timezone)
                tmp_list.append(index)
            except:
                pass
        df = df[df.index.isin(tmp_list)]
        df.index = df.index.tz_localize(timezone)
        df = df.sort_index()
        df = df[~df.index.duplicated(keep='first')]
        df = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1], freq='1min')).fillna(0)
        df['GHI'] = df['Global PSP [W/m^2]']
        try:
            df['GHI'].fillna(df['Global PSP (cor) [W/m^2]'], inplace=True)
        except:
            df['GHI'] = df['GHI'].fillna(0)
        df.index = df.index.tz_convert('UTC')
        return cls(df[keepers])

    @classmethod
    def read_ornl_file(cls, filename, timezone='EST', keepers = ['GHI']):
        df = pd.read_csv(filename)
        df.index = pd.to_datetime(df['DATE (MM/DD/YYYY)'] + ' ' + df['EST'], format='%m/%d/%Y %H:%M')
        tmp_list = []
        for index in df.index:
            try:
                index.tz_localize(timezone)
                tmp_list.append(index)
            except:
                pass
        df = df[df.index.isin(tmp_list)]
        df.index = df.index.tz_localize(timezone)
        df = df.sort_index()
        df = df[~df.index.duplicated(keep='first')]
        df = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1], freq='1min')).fillna(0)
        df['GHI'] = df['Global Horizontal [W/m^2]']
        return cls(df[keepers])

    @classmethod
    def read_pickle(cls, filename, compression=None):
        """Read dataframe from pickle file.

        Arguments
        ---------
        filename: str
            Name of pickle file.
        """
        df = pd.read_pickle(filename)
        return cls(df)

    # def __trim_bad_indices_tz(self, indices, timezone):
    #     tmp_list = []
    #     for index in indices:
    #         try:
    #             index.tz_localize(timezone)
    #             tmp_list.append(index)
    #         except:
    #             pass
    #     return tmp_list

    def to_pickle(self, filename, overwrite=False, compression=None):
        """Dump dataframe to pickle file.

        Arguments
        ---------
        filename: str
            Name of pickle file.
        overwrite, optional: bool
            Overwrite file if it exists.
        """
        if os.path.exists(filename) and not overwrite:
            raise FileExistsError('The specified file already exists.  Change name or set overwrite.')
        self.df.to_pickle(filename)

    def robust_rolling_smooth(self, col, window, label=None, overwrite=False):
        """Smooth column of data robustly (median + mean).

        Arguments
        ---------
        col: str
            Column in dataframe.
        window: int
            Number of points to include in window.
        label, optional: str
            Name of smoothed column.
        overwrite, optional: bool
            Overwrite new column or not.
        """
        if label is None:
            label = col + ' smooth'
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Provide label name or allow for overwrite.')
        self.df[label] = \
            self.df[col].rolling(window, center=True).median().fillna(0).rolling(window, center=True).mean().fillna(0)

    def trim_dates(self, x0=None, x1=None):
        """Trim date range of dataframe.

        The x0 is inclusive, x1 is exclusive.  If either are None, assume no trimming in said direction.

        Arguments
        ---------
        x0, optional: datetime or str
            Earlier date (inclusive).
        x1, optional: datetime or str
            Later date (exclusive).
        """
        if x0 is not None:
            self.df = self.df[self.df.index >= x0]
        if x1 is not None:
            self.df = self.df[self.df.index < x1]

    def intersection(self, other_indices):
        """Modify indices such that dataframe intersects with different set of indices.

        Arguments
        ---------
        other_indices: pd.DateTimeIndex
            Indices to intersect with.
        """
        indices = self.df.index.intersection(other_indices)
        self.df = self.df[self.df.index.isin(indices)]

    def by_time_of_day_transform(self, data='GHI'):
        """Transform time series (linear) to matrix representation.

        The dataframe will be organized as Date (columnss) X Timestample (rows).

        Arguments
        ---------
        data, optional: str
            Which data column (nsrdb/ground).  Assumed to be 'GHI'.

        Returns
        -------
        by_time: pd.DataFrame
            DataFrame with time of day as rows and dates as columns.
        """
        by_time = pd.pivot_table(self.df, index=self.df.index.time, columns=self.df.index.date, values=data)
        return by_time

    def generate_statistical_clearsky(self, data='GHI', num_days=30,
                                      model_fxn=np.nanpercentile,
                                      percentile=90, smooth_window=0, smooth_fxn=None,
                                      label='Clearsky GHI stat', overwrite=False):
        """Generate statistical clearsky for either dataset.

        Arguments
        ---------
        data, optional: str
            Data column to generate clearsky, assumed to be 'GHI'.
        num_days, optional: int
            Size of window (in days) for constructing clearsky curve.
        model_fxn, optional: callable
            Function for determining clearsky curve.
        percentile, optional: float
            Perctile value used in statistical clearsky, ignored if model_fxn is not np.percentile or np.nanpercentile.
        smooth_window, optional: int
            Size (number of data points) for smoothing the statistical clear sky.
        smooth_fxn, optional: callable
            Function for smoothing the clear sky curve.
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        # old_tz = self.df.index.tz
        # old_idx = self.df.index
        # self.df.index = self.df.index.tz_convert('UTC')
        by_time = self.by_time_of_day_transform(data)
        dates = self.df.index.date

        stat_cs_list = []
        for day_to_filter, sample in self.generate_day_range(dates, num_days):
            sample_days = by_time[sample]
            stat_cs = self.stat_cs_per_sample(dates, day_to_filter, sample_days,
                                              model_fxn, percentile)
            stat_cs_list.append(stat_cs)

        stat_cs_ser = pd.concat(stat_cs_list, axis=0)
        stat_cs_ser.index = self.df.index

        if smooth_window > 0 and smooth_fxn is not None:
            stat_cs_ser = stat_cs_ser.rolling(smooth_window, center=True).\
                apply(smooth_fxn).fillna(0)

        self.df[label] = stat_cs_ser
        # self.df.index = self.df.index.tz_convert(old_tz)
        # self.df.index = old_idx

    def stat_cs_per_sample(self, dates, day_to_filter, sample_days,
                           model_fxn=np.nanpercentile, percentile=90):
        """Filter measurements by time of day based on deviation from fxn.

        Arguments
        ---------
        dates: list-like
            Unique dates in data.
        day_to_filter: datetime.date
            Day which will be filtered.
        sample_days: pd.DataFrame
            DataFrame of measured values (rows are time of day and columns are dates).
        model_fxn, optional: callable
            Function that will be used to construct the statistical clearsky curve.
        percentile, optional: float
            Percentile value for clearsky curve construction if percentile based function used.
        """
        # fixing indices is important - the by_time_of_day_transform will fill daylight savings/etc
        # which confuses indexing central vals
        correct_indices = self.df.loc[dates == day_to_filter].index
        correct_times = correct_indices.time
        sample_days = sample_days[sample_days.index.isin(correct_times)]

        if model_fxn in (np.percentile, np.nanpercentile):
            args = ([percentile])
        else:
            args = ()

        central_vals = pd.Series(sample_days.replace(0, np.nan).apply(model_fxn, axis=1, args=args),
                                 name='central').fillna(0)
        central_vals.index = correct_indices

        return central_vals

    def generate_day_range(self, dates, num_days=30):
        """Generates groups of days for statistical analysis.

        Arguments
        ---------
        window_size, optional: int
            Size of window (in days).
        dates: list-like
            Unique dates in data.

        Yields
        ------
        day_range: tuple
            (Day of interest, date of days (+/- window_size / 2))
        """
        if num_days > 31:
            warnings.warn('Using a large window of days may give suspect results.', RuntimeWarning)
        if num_days < 3:
            warnings.warn('Using a very small window of days give suspect results.', RuntimeWarning)

        days = pd.unique(dates)
        if len(days) <= num_days:
            warnings.warn('Data is smaller than specified window size.', RuntimeWarning)
            for i in range(len(days)):
                yield days[i], days
        else:
            plus_minus = (num_days // 2) + 1
            for i in range(len(days)):
                if i - plus_minus < 0:
                    day_range = days[:num_days]
                elif i + plus_minus >= len(days):
                    day_range = days[len(days) - num_days: len(days)]
                else:
                    day_range = days[i - plus_minus + 1: i + plus_minus]
                yield days[i], day_range

    def generate_pvlib_clearsky(self, latitude, longitude, altitude=None, label='Clearsky GHI pvlib', overwrite=False):
        """Generate pvlib clearsky curve.

        Arguments
        ---------
        lat: float
            Latitutde.
        lon: float
            Longitude.
        elevation, optional: float
            Elevation of system (this is not required by Location object).
        label: str
            Label of clearsky data points.
        overwrite: bool
            Allow column to be overwritten if exists.
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        loc = pvlib.location.Location(latitude, longitude, altitude=altitude)
        clear_skies = loc.get_clearsky(self.df.index)
        clear_skies = pd.Series(clear_skies['ghi'])
        clear_skies.index = self.df.index
        self.df[label] = clear_skies

    # def calc_distance_to_solar_noon(self, model_col, label='time_to_solarnoon', overwrite=False):
    #     """Get solar position based on indices and optionally calculate distance to solar noon for each part of the day.
    #
    #     There is probably a faster way to do this.
    #
    #     Arguments
    #     ---------
    #     model_col: str
    #         Column of model GHI.
    #     label, optional: str
    #         Name of calculated time to solar noon.
    #     overwrite, optional: bool
    #         Overwrite label if it exists.
    #     """
    #     if label in self.df.keys() and not overwrite:
    #         raise RuntimeError('Label already exists. Set overwrite to true or set label name.')
    #     max_indices = []
    #     for _, group in self.df[model_col].groupby(self.df[model_col].index.date):
    #         max_indices.append(group.idxmax())
    #     max_indices = pd.DataFrame(max_indices)
    #     max_indices.index = max_indices.values
    #     res_list = []
    #     # for idx in max_indices.index:
    #     #     res = self.df.iloc[self.df.index.get_loc(idx, method='nearest')]
    #     #     res_list.append(res)
    #     # return res_list
    #     for idx in self.df.index:
    #         res = max_indices.iloc[max_indices.index.get_loc(idx, method='nearest')]
    #         res_list.append(res)
    #     nearest_max = pd.DataFrame(res_list, columns=['nearest_max'])
    #     self.df[label] = np.abs(self.df.index - nearest_max['nearest_max'].values).total_seconds() / 60
    #     # return res_lis
    #
    #
    # #     # dist_list = []
    # #     # for idx in self.df.index:
    # #     #     # subset = [i for i in max_indices if i in pd.date_range(start=idx - pd.Timedelta('1 days'), end=idx + pd.Timedelta('1 days'), freq=self.df.index.freq)]
    # #     #     date_range = [(idx - pd.Timedelta('1 days')).date(), (idx + pd.Timedelta('1 days')).date(), idx.date()]
    # #     #     subset = np.asarray([i for i in max_indices if i.date() in date_range])
    # #     #     distances = [np.abs((idx - i).total_seconds() / 60) for i in subset]
    # #     #     dist_list.append(np.min(distances))

    def pvlib_clearsky_detect(self, scale=False, measured='GHI',
                              modeled='Clearsky GHI pvlib', label='sky_status pvlib', window=10, overwrite=False):
        """Detect clearsky using PVLib methods.

        Arguments
        ---------
        scale, optional: bool
            Scale modeled column by factor determined by PVLib.
        measured, optional: str
            Column to detect.
        modeled, optional: str
            Column of modeled clearsky (reference for measured).
        result, optional: str
            Column name of results.
        window, optional: int
            Size of window for PVLib detection.
        overwrite, optional: bool
            Allow column to be overwritten if label exists.
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        alpha = 1.0
        if scale:
            status, _, alpha = \
                pvlib.clearsky.detect_clearsky(self.df[measured], self.df[modeled],
                                               self.df.index, window, return_components=True)
        else:
            status = pvlib.clearsky.detect_clearsky(self.df[measured], self.df[modeled], self.df.index, window)
        self.df[label] = status
        if scale:
            self.df[modeled + ' scaled'] = alpha * self.df[modeled]

    def set_nsrdb_sky_status(self, label='sky_status', overwrite=False):
        """Set sky status target for machine learning applications for nsrdb data.

        Arguments
        ---------
        label, optional: str
            Column name for new sky status.
        overwrite, optional: bool
            Allow column to be overwritten in column exists
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        self.df[label] = (self.df['Cloud Type'] == 0) & (self.df['GHI'] > 0)

    def calc_abs_ratio_diff(self, col1, col2, label='abs_diff_ratio', overwrite=False):
        """Absolute difference of the ratio of two series from 1.

        Arguments
        ---------
        col1: str
            Column for numerator.
        col2: str
            Column for denominator.
        label: str
            Name of resultant column.
        overwrite, optional: bool
            Overwite column if it exists.
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        ratio = (self.df[col1] / self.df[col2]).replace([-np.inf, np.inf, np.nan], 1)
        self.df[label] = np.abs(1 - ratio)

    def calc_line_length(self, col, window, dx, label=None, overwrite=False):
        """Calculate the rolling line length of data series.

        Arguments
        ---------
        col: str
            Data column to calculate for.
        window: int
            Number of time periods to include in window.
        dx: float
            Delta of x values (time in this case).
        label, optional: str
            Name of resultant column.  If None, will add 'line length' to col.
        overwrite, optional: bool
            Overwrite label if it exists.
        """
        if label is None:
            label = col + ' line length'
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        ser = self.df[col].rolling(window, center=True).apply(lambda x: self.line_length(x, dx)).fillna(0)
        self.df[label] = ser

    def line_length(self, x, dx):
        """Calculate line length traveled by array x (with delta x = dx).

        Arguments
        ---------
        x: np.ndarray
            1 dimensional array of numeric values.
        dx: float
            Difference of x values.

        Returns
        -------
        length: float
            Distsance curve travels.
        """
        ydiffs = np.diff(x)
        xdiffs = np.asarray([dx for i in range(len(ydiffs))])
        length = np.sum(np.sqrt((ydiffs) + (xdiffs)))
        return length

    def calc_ratio(self, col1, col2, label='ratio', overwrite=False):
        """Ratio of two columns of dataframe.

        Arguments
        ---------
        col1: str
            Column for numerator.
        col2: str
            Column for denominator.
        label: str
            Name of resultant column.
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        ratio = (self.df[col1] / self.df[col2]).replace([-np.inf, np.inf, np.nan], 0)  # replace with 1 for 0/0 case
        self.df[label] = ratio

    def calc_abs_diff(self, col1, col2, label='abs_diff', overwrite=False):
        """Absolute difference of two columns.

        Arguments
        ---------
        col1: str
            Column for numerator.
        col2: str
            Column for denominator.
        label: str
            Name of resultant column.
        """
        if label in self.df.keys() and not overwrite:
            raise RuntimeError('Label already exists.  Set overwrite to True or pick new label name.')
        absdiff = np.abs(self.df[col1] - self.df[col2])  # replace with 1 for 0/0 case
        self.df[label] = absdiff

    def calc_window_stat(self, window, col, label, overwrite=False):
        """Calculates window-based statistics on columns.

        Arguments
        ---------
        window: int
            Size of window for calculating metrics.  Defined as the number of timesteps to use in the window.
            For example, if data is 30 min frequency and window=3, the window will span 1 hour.
        col: str
            Which column to use.
        label: str
            Result column label.
        overwrite, optional: bool
            Allow columns to be overwritten if they exist.
        """
        test_labels = [label + i for i in [' mean', ' std', ' max', ' min', ' range']]
        if any(i in self.df.keys() for i in test_labels) and not overwrite:
            raise RuntimeError('A label already exists.  Use new label name set overwrite to True.')
        # self.df[label + ' mean'] = self.df[col].rolling(window, center=True).mean().fillna(0)
        # self.df[label + ' std']  = (self.df[col].rolling(window, center=True).std().fillna(0) / self.df[label + ' mean']).replace([-np.inf, np.inf, np.nan], 0)
        # self.df[label + ' max']  = (self.df[col].rolling(window, center=True).max().fillna(0) / self.df[label + ' mean']).replace([-np.inf, np.inf, np.nan], 1)
        # self.df[label + ' min']  = (self.df[col].rolling(window, center=True).min().fillna(0) / self.df[label + ' mean']).replace([-np.inf, np.inf, np.nan], 1)
        # self.df[label + ' range']  = ((self.df[label + ' max'] - self.df[label + ' min']) / self.df[label + ' mean']).replace([-np.inf, np.inf, np.nan], 0)
        self.df[label + ' mean'] = self.df[col].rolling(window, center=True).mean().fillna(0)
        self.df[label + ' std']  = self.df[col].rolling(window, center=True).std().fillna(0)
        self.df[label + ' max']  = self.df[col].rolling(window, center=True).max().fillna(0)
        self.df[label + ' min']  = self.df[col].rolling(window, center=True).min().fillna(0)
        self.df[label + ' range']  = self.df[label + ' max'] - self.df[label + ' min']

    def calc_all_window_metrics(self, window, dx, col1='GHI', col2='Clearsky GHI',
                                abs_ideal_ratio_diff='abs_ideal_ratio_diff', abs_diff='abs_diff', overwrite=False):
        """Setup dataframes for machine learning.

        Arguments
        ---------
        window: int
            Size of window for statistics calculations.
        dx: float
            Step size for gradient calculation.
        col1: str
            Numerator for ratio and abs_diff_ratio calculation.
        col2: str
            Denominator for ratio and abs_diff_ratio calculation.
        ratio_label: str
            Label for ratio columns.
        abs_ratio_diff_label: str
            Label for absolute difference ratio columns.
        overwrite, optional: bool
            Allow column to be overwritten.
        """
        if (abs_ideal_ratio_diff in self.df.keys()) and not overwrite:
            raise RuntimeError('A label already exists.  Set overwrite to True or pick new label name.')
        dx = self.df.index.freq.nanos / 1.0e9 / 60.0e0

        # GHI, GHIcs window stats
        self.calc_window_stat(window, col1, col1, overwrite=overwrite)
        self.calc_window_stat(window, col2, col2, overwrite=overwrite)

        # dGHI/dt, d2GHI/dt2
        label = col1 + ' gradient'
        # self.df[label] = self.df[col1].diff().fillna(0) / dx
        self.df[label] = np.gradient(self.df[col1], dx)
        self.calc_window_stat(window, label, label, overwrite=overwrite)
        label2 = label + ' second'
        # self.df[label2] = self.df[label].diff().fillna(0) / dx
        self.df[label2] = np.gradient(self.df[label], dx)
        self.calc_window_stat(window, label2, label2, overwrite=overwrite)

        # dGHIcs/dt, dGHIcs2/dt2
        label = col2 + ' gradient'
        # self.df[label] = self.df[col2].diff().fillna(0) / dx
        self.df[label] = np.gradient(self.df[col2], dx)
        self.calc_window_stat(window, label, label, overwrite=overwrite)
        label2 = label + ' second'
        # self.df[label2] = self.df[label].diff().fillna(0) / dx
        self.df[label2] = np.gradient(self.df[label], dx)
        self.calc_window_stat(window, label2, label2, overwrite=overwrite)

        # |1 - GHI/GHIcs|
        self.calc_abs_ratio_diff(col1, col2, abs_ideal_ratio_diff, overwrite=overwrite)
        self.df[abs_ideal_ratio_diff] = self.df[abs_ideal_ratio_diff]
        self.calc_window_stat(window, abs_ideal_ratio_diff, abs_ideal_ratio_diff, overwrite=overwrite)
        label = abs_ideal_ratio_diff + ' pct_change'
        self.df[label] = self.df[abs_ideal_ratio_diff].pct_change().fillna(0)
        self.calc_window_stat(window, label, label, overwrite=overwrite)

        # d|1 - GHI/GHIcs|/dt, d2|1 - GHI/GHIcs|/dt2
        label = abs_ideal_ratio_diff + ' gradient'
        # self.df[label] = self.df[abs_ideal_ratio_diff].diff().fillna(0) / dx
        self.df[label] = np.gradient(self.df[abs_ideal_ratio_diff], dx)
        self.calc_window_stat(window, label, label, overwrite=overwrite)
        label2 = label + ' second'
        # self.df[label2] = self.df[label].diff().fillna(0) / dx
        self.df[label2] = np.gradient(self.df[label], dx)
        self.calc_window_stat(window, label2, label2, overwrite=overwrite)

        # # |GHI - GHIcs|
        # self.calc_abs_diff(col1, col2, abs_diff, overwrite=overwrite)
        # self.calc_window_stat(window, abs_diff, abs_diff, overwrite=overwrite)

        # # d|GHI - GHIcs|/dt, d2|GHI - GHIcs|/dt2
        # label = abs_diff + ' gradient'
        # self.df[label] = np.gradient(self.df[abs_diff], dx)
        # self.calc_window_stat(window, label, label, overwrite=overwrite)
        # label2 = label + ' second'
        # self.df[label2] = np.gradient(self.df[label], dx)
        # self.calc_window_stat(window, label2, label2, overwrite=overwrite)

        # \1 - (dGHI / dt) / (dGHIcs / dt)|, \1 - (d2GHI / dt2) / (d2GHIcs / dt2)|
        self.calc_abs_ratio_diff(col1 + ' gradient', col2 + ' gradient', col1 + ' ' + col2 + ' gradient ratio', overwrite=overwrite)
        self.calc_window_stat(window, col1 + ' ' + col2 + ' gradient ratio', col1 + ' ' + col2 + ' gradient ratio', overwrite=overwrite)
        self.calc_abs_ratio_diff(col1 + ' gradient second', col2 + ' gradient second', col1 + ' ' + col2 + ' gradient second ratio', overwrite=overwrite)
        self.calc_window_stat(window, col1 + ' ' + col2 + ' gradient second ratio', col1 + ' ' + col2 + ' gradient second ratio', overwrite=overwrite)
        # self.calc_window_stat(window, abs_ideal_ratio_diff, abs_ideal_ratio_diff, overwrite=overwrite)

        # GHI line length
        label_ll_1 = col1 + ' line length'
        self.calc_line_length(col1, window, dx, label=label_ll_1, overwrite=overwrite)

        # GHIcs line length
        label_ll_2 = col2 + ' line length'
        self.calc_line_length(col2, window, dx, label=label_ll_2, overwrite=overwrite)

        # |1 - GHI LL/GHIcs LL|
        self.calc_abs_ratio_diff(label_ll_1, label_ll_2, abs_ideal_ratio_diff + ' line length', overwrite=overwrite)

        # GHI LL / GHIcs LL, d(GHI LL / GHIcs LL) / dt
        label = col1 + ' ' + col2 + ' line length ratio'
        self.calc_ratio(label_ll_1, label_ll_2, label, overwrite=overwrite)
        label1 = label + ' gradient'
        self.df[label1] = np.gradient(self.df[label], dx)
        label2 = label1 + ' second'
        self.df[label2] = np.gradient(self.df[label1], dx)

        # # GHI diff
        # self.df[col1 + ' diff'] = np.abs(self.df[col1].diff()).fillna(0)
        #
        # # GHIcs diff
        # self.df[col2 + ' diff'] = np.abs(self.df[col2].diff()).fillna(0)
        #
        # # |1 - GHI diff / GHIcs diff|
        # self.calc_abs_ratio_diff(col1 + ' fdiff', col2 + ' fdiff', col1 + ' ' + col2 + ' fdiff ratio', overwrite=overwrite)
        # self.calc_window_stat(window, col1 + ' ' + col2 + ' fdiff ratio',
        #                               col1 + ' ' + col2 + ' fdiff ratio', overwrite=overwrite)
        # self.calc_abs_ratio_diff(col1 + ' bdiff', col2 + ' bdiff', col1 + ' ' + col2 + ' bdiff ratio', overwrite=overwrite)
        # self.calc_window_stat(window, col1 + ' ' + col2 + ' bdiff ratio',
        #                               col1 + ' ' + col2 + ' bdiff ratio', overwrite=overwrite)
        #
        # GHI pct change
        # self.df[col1 + ' bpct change'] = self.df[col1].pct_change().replace([-np.inf, np.inf, np.nan], 0)
        # self.calc_window_stat(window, col1 + ' bpct change', col1 + ' bpct change', overwrite=overwrite)
        # self.df[col1 + ' fpct change'] = np.abs(self.df[col1].pct_change(-1).replace([-np.inf, np.inf, np.nan], 100))
        # self.calc_window_stat(window, col1 + ' fpct change', col1 + ' fpct change', overwrite=overwrite)
        #
        # # GHIcs pct change
        # self.df[col2 + ' bpct change'] = self.df[col2].pct_change().replace([-np.inf, np.inf, np.nan], 0)
        # self.calc_window_stat(window, col2 + ' bpct change', col2 + ' bpct change', overwrite=overwrite)
        # self.df[col2 + ' fpct change'] = np.abs(self.df[col2].pct_change(-1).replace([-np.inf, np.inf, np.nan], 100))
        # self.calc_window_stat(window, col2 + ' fpct change', col2 + ' fpct change', overwrite=overwrite)
        #
        # self.calc_abs_ratio_diff(col1 + ' fpct change', col2 + ' fpct change', col1 + ' ' + col2 + ' fpct change ratio', overwrite=overwrite)
        # self.calc_window_stat(window, col1 + ' ' + col2 + ' fpct change ratio', col1 + ' ' + col2 + ' fpct change ratio', overwrite=overwrite)
        # self.calc_ratio(col1 + ' bpct change', col2 + ' bpct change', col1 + ' ' + col2 + ' bpct change ratio', overwrite=overwrite)
        # self.calc_window_stat(window, col1 + ' ' + col2 + ' bpct change ratio', col1 + ' ' + col2 + ' bpct change ratio', overwrite=overwrite)

    # def fit_model(self, feature_cols, target_cols, clf,
    #               conf_matrix=True, cv=True):
    #     """Fit machine learning model based on data frame.
    #
    #     Arguments
    #     ---------
    #     feature_cols: list-like
    #         Column names to use as features in ML model.
    #     target_cols: list-like
    #         Column names to use as target label.
    #     clf: sklearn estimator
    #         Object with fit and predict methods.
    #     train_test_split, optional: bool
    #         Perform train_test_split.
    #     cv, optional: bool
    #         Perform cross validation.
    #     """
    #     X = self.df[feature_cols].values
    #     y = self.df[target_cols].values.flatten()
    #     X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)
    #     if cv:
    #         scores = model_selection.cross_val_score(clf, X_train, y_train)
    #         mean_score = np.round(np.mean(scores), 4)
    #         std_score = np.round(np.std(scores), 4)
    #         print('CV score: {} +/- {}'.format(mean_score, std_score))
    #     clf.fit(X_train, y_train)
    #     y_pred = clf.predict(X_test)
    #     score = metrics.accuracy_score(y_test, y_pred)
    #     print('Train/test split score: {}'.format(score))
    #     return clf

    def scale_model(self, meas_col, model_col, status_col=None):
        """Scale model values to measured values based on clear sky periods.

        Arguments
        ---------
        meas_col: str
            Column of measured GHI values.
        model_col: str
            Column of model GHI values.
        status_col: str
            Column of clear/cloudy classification (must be binary).
        """
        if status_col is not None:
            clear_meas = self.df[self.df[status_col]][meas_col]
            clear_model = self.df[self.df[status_col]][model_col]
        else:
            clear_meas = self.df[self.df[status_col]]
            clear_model = self.df[self.df[status_col]]
        alpha = 1
        def rmse(alpha):
            sqr_err = (clear_meas - (alpha * clear_model))**2
            return np.sqrt(np.mean(sqr_err))
        alpha = optimize.minimize_scalar(rmse).x
        self.df[model_col] = alpha * self.df[model_col]

    def iter_predict(self, feature_cols, meas_col, model_col, clf, window, n_iter=20, tol=1.0e-8, ml_label='sky_status iter', smooth=False, overwrite=True):
        """Predict clarity based using classifier that iteratively fits the model column
        to the measured column based on clear points.

        This function WILL overwrite columns that already exist in the data frame.

        Arguments
        ---------
        feature_cols: list-like
            Column names to use as features in ML model.
        clf: sklearn estimator
            Object with fit and predict methods.
        meas_col: str
            Column of measured data.
        model_col: str
            Column of model data.
        n_iter, optional: int
            Number of iterations for fitting model to measured column.
        tol, optoinal: float
            Criterion for convergence of modeled and measured clear points.
        ml_label, optional: str
            Label for predicted clear/cloudy points.
        smooth, optional: bool
            Smooth results.  Smoothing is aggressive as a point must be clear
            in every window it appears in.
        overwrite, optional: bool
            Permission to overwrite columns if they exist.
        """
        alpha = 1
        for it in range(n_iter):
            print(it + 1, alpha)
            self.calc_all_window_metrics(window, None, col1=meas_col, col2=model_col,
                                         ratio_label='ratio', abs_ratio_diff_label='abs_diff_ratio', overwrite=overwrite)
            X = self.df[feature_cols].values
            y_pred = clf.predict(X)
            clear_meas = self.df[y_pred][meas_col]
            clear_model = self.df[y_pred][model_col]
            alpha_last = alpha
            def rmse(alpha):
                return np.sqrt(np.mean((clear_meas - (alpha * clear_model))**2))
            min_scalar = optimize.minimize_scalar(rmse)
            alpha = min_scalar.x
            if np.abs(alpha - alpha_last) < tol:
                break
            self.df[model_col] = self.df[model_col] * alpha
        if it == n_iter - 1:
            warnings.warn('Scaling did not converge.', RuntimeWarning)
        self.df[ml_label] = y_pred
        if smooth:
            self.smooth_ml_label(window * 2, ml_label)
        return self.df[ml_label]

    def iter_predict_daily(self, feature_cols, meas_col, model_col, clf, window, n_iter=20,
                                 tol=1.0e-8, ml_label='sky_status iter', smooth=False, overwrite=True):
        """Predict clarity based using classifier that iteratively fits the model column
        to the measured column based on clear points.

        This method differs from iter_predict method because it predicts/scales on individual days, not the entire data set.
        This function WILL overwrite columns that already exist in the data frame.

        Method should be refactored.  Calculating features and scaling model GHI is done across entire data set even though we only need one day at a time.

        Arguments
        ---------
        feature_cols: list-like
            Column names to use as features in ML model.
        clf: sklearn estimator
            Object with fit and predict methods.
        meas_col: str
            Column of measured data.
        model_col: str
            Column of model data.
        n_iter, optional: int
            Number of iterations for fitting model to measured column.
        tol, optoinal: float
            Criterion for convergence of modeled and measured clear points.
        ml_label, optional: str
            Label for predicted clear/cloudy points.
        smooth, optional: bool
            Smooth results.  Smoothing is aggressive as a point must be clear
            in every window it appears in.
        overwrite, optional: bool
            Permission to overwrite columns if they exist.
        """
        day_preds = []
        for name, day in self.df.groupby(self.df.index.date):
            print(name)
            alpha = 1
            running_alpha = 1
            converged = False
            for _ in range(n_iter):
                self.calc_all_window_metrics(window, None, col1=meas_col, col2=model_col, overwrite=overwrite)
                X = self.df[self.df.index.isin(day.index)][feature_cols].values
                y_pred = clf.predict(X)
                clear_meas = self.df[self.df.index.isin(day.index)][y_pred][meas_col]
                clear_model = self.df[self.df.index.isin(day.index)][y_pred][model_col]
                alpha_last = alpha
                def rmse(alpha):
                    sqr_err = (clear_meas - (alpha*clear_model))**2
                    return np.sqrt(np.mean(sqr_err))
                min_scalar = optimize.minimize_scalar(rmse)
                alpha = min_scalar.x
                if np.abs(alpha - alpha_last) < tol:
                    converged = True
                    break
                self.df[model_col] = self.df[model_col] * alpha
                running_alpha *= alpha
            self.df[model_col] = self.df[model_col] / running_alpha
            if not converged:
                warnings.warn('Scaling did not converge.', RuntimeWarning)
            day_preds.append(y_pred)
        self.df[ml_label] = np.append(np.asarray([]), day_preds)
        if smooth:
            self.smooth_ml_label(window * 2, ml_label)
        return self.df[ml_label]

    def smooth_ml_label(self, window, label):
        """Smooth labeled points.

        Labeled points are smooth by assuring that each point in a window is labeled as clear.
        This is only appropriate for binary data.
        """
        ser = self.df[label].rolling(window, center=True).apply(lambda x: np.sum(x) == len(x)).fillna(0)
        self.df[label] = ser

    def label_smoother(self, x):
        return np.sum(x) == len(x)
        midpt = len(x) // 2
        return np.sum(x[:midpt]) >= midpt or np.sum(x[midpt:]) >= midpt

    def time_from_solar_noon(self, col, label):
        """Calculate distance from solar noon (absolute) in minutes.

        Solar noon is defined as the index of the peak of the self.df[col] data.

        Arguments
        ---------
        col: str
            Column to be used for solar noon peak finding.
        label:
            Resultant label.
        """
        mins = []
        for name, day in self.df.groupby(self.df.index.date):
            maxidx = day[col].idxmax()
            mins.append(np.asarray((day.index - maxidx).total_seconds()) / 60.0e0)
        mins = np.asarray(mins)
        self.df[label] = mins.flatten()
